torch>=2.0.0
torchvision>=0.15.0
insightface>=0.7.3
onnxruntime-gpu>=1.15.0
fastapi>=0.100.0
uvicorn[standard]>=0.23.0
python-multipart>=0.0.6
Pillow>=10.0.0
numpy>=1.24.0
opencv-python>=4.8.0
kornia>=0.7.0
tqdm>=4.65.0
pydantic>=2.0.0

# v2 pipeline (optional but recommended)
lpips>=0.1.4                # Perceptual loss — gold standard
open_clip_torch>=2.20.0     # CLIP ViT-H/14 for dual-targeting
# transformers>=4.30.0      # Fallback CLIP backend (if open_clip unavailable)

# Cross-model ensemble (optional — improves transferability)
facenet-pytorch>=2.5.3      # FaceNet InceptionResNet-V1 for ensemble attack

# Deepfake tool testing (for testing protection against real deepfake pipelines)
diffusers>=0.25.0           # IP-Adapter FaceID Plus v2 (Stable Diffusion)
transformers>=4.30.0        # Required by diffusers for CLIP
accelerate>=0.25.0          # Required by diffusers for model loading
peft>=0.7.0                 # Required for LoRA weight loading
